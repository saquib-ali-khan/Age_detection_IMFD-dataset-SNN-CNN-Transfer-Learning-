{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deepak/Saquib/CNNs\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age_detection.ipynb\r\n",
      "Age_detection_sub1.csv\r\n",
      "Age_detection_sub2.csv\r\n",
      "Converting MNIST images to RGB.ipynb\r\n",
      "data\r\n",
      "first_try.h5\r\n",
      "mnist.pkl.gz\r\n",
      "Rough Notebook.ipynb\r\n",
      "sub1.csv\r\n",
      "sub2.csv\r\n",
      "sub3.csv\r\n",
      "Transfer Learning.ipynb\r\n",
      "vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age_detection.ipynb\r\n",
      "Age_detection_sub1.csv\r\n",
      "Age_detection_sub2.csv\r\n",
      "Converting MNIST images to RGB.ipynb\r\n",
      "data\r\n",
      "first_try.h5\r\n",
      "mnist.pkl.gz\r\n",
      "Rough Notebook.ipynb\r\n",
      "sub1.csv\r\n",
      "sub2.csv\r\n",
      "sub3.csv\r\n",
      "Transfer Learning.ipynb\r\n",
      "vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age_detection.ipynb\r\n",
      "Age_detection_sub1.csv\r\n",
      "Age_detection_sub2.csv\r\n",
      "Converting MNIST images to RGB.ipynb\r\n",
      "data\r\n",
      "first_try.h5\r\n",
      "mnist.pkl.gz\r\n",
      "Rough Notebook.ipynb\r\n",
      "sub1.csv\r\n",
      "sub2.csv\r\n",
      "sub3.csv\r\n",
      "Transfer Learning.ipynb\r\n",
      "vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv('data/train/train.csv')\n",
    "test = pd.read_csv('data/test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=19906, step=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.range.RangeIndex"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19906</td>\n",
       "      <td>19906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>25628.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID   Class\n",
       "count       19906   19906\n",
       "unique      19906       3\n",
       "top     25628.jpg  MIDDLE\n",
       "freq            1   10804"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of               ID   Class\n",
       "0        377.jpg  MIDDLE\n",
       "1      17814.jpg   YOUNG\n",
       "2      21283.jpg  MIDDLE\n",
       "3      16496.jpg   YOUNG\n",
       "4       4487.jpg  MIDDLE\n",
       "5       6283.jpg  MIDDLE\n",
       "6      23495.jpg   YOUNG\n",
       "7       7100.jpg   YOUNG\n",
       "8       6028.jpg   YOUNG\n",
       "9      22617.jpg     OLD\n",
       "10     11177.jpg   YOUNG\n",
       "11      2462.jpg  MIDDLE\n",
       "12     24116.jpg  MIDDLE\n",
       "13     17689.jpg  MIDDLE\n",
       "14       728.jpg  MIDDLE\n",
       "15      3003.jpg  MIDDLE\n",
       "16     14408.jpg     OLD\n",
       "17      6679.jpg   YOUNG\n",
       "18     15152.jpg     OLD\n",
       "19     24784.jpg  MIDDLE\n",
       "20      9970.jpg   YOUNG\n",
       "21     22550.jpg     OLD\n",
       "22       150.jpg   YOUNG\n",
       "23      7379.jpg  MIDDLE\n",
       "24     15387.jpg  MIDDLE\n",
       "25      2336.jpg   YOUNG\n",
       "26      9603.jpg  MIDDLE\n",
       "27      4025.jpg     OLD\n",
       "28     17696.jpg  MIDDLE\n",
       "29     17552.jpg   YOUNG\n",
       "...          ...     ...\n",
       "19876  11988.jpg     OLD\n",
       "19877   9407.jpg     OLD\n",
       "19878  25426.jpg   YOUNG\n",
       "19879  16609.jpg   YOUNG\n",
       "19880  18746.jpg   YOUNG\n",
       "19881  25714.jpg  MIDDLE\n",
       "19882  14939.jpg  MIDDLE\n",
       "19883  10025.jpg   YOUNG\n",
       "19884   6149.jpg     OLD\n",
       "19885   9733.jpg   YOUNG\n",
       "19886  22630.jpg  MIDDLE\n",
       "19887  11803.jpg  MIDDLE\n",
       "19888  10812.jpg   YOUNG\n",
       "19889   7038.jpg   YOUNG\n",
       "19890   3021.jpg  MIDDLE\n",
       "19891   4727.jpg   YOUNG\n",
       "19892   7979.jpg  MIDDLE\n",
       "19893  26159.jpg  MIDDLE\n",
       "19894   2040.jpg  MIDDLE\n",
       "19895  25347.jpg  MIDDLE\n",
       "19896  15100.jpg   YOUNG\n",
       "19897  26182.jpg  MIDDLE\n",
       "19898    742.jpg     OLD\n",
       "19899   5318.jpg  MIDDLE\n",
       "19900  25514.jpg  MIDDLE\n",
       "19901   2482.jpg  MIDDLE\n",
       "19902  20085.jpg   YOUNG\n",
       "19903  19663.jpg  MIDDLE\n",
       "19904  10132.jpg  MIDDLE\n",
       "19905   9896.jpg  MIDDLE\n",
       "\n",
       "[19906 rows x 2 columns]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=19906, step=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=19906, step=1)\n"
     ]
    }
   ],
   "source": [
    "print train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = random.choice(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8305\n"
     ]
    }
   ],
   "source": [
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = random.choice(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6234\n"
     ]
    }
   ],
   "source": [
    "print i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_name = train.ID[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################## The train object or dataframe right now only contains the names of images, not the images themselves,i.e, not their pixel values.#############\n",
    "####  train rt now does not contain the real dataset. it just contains it's reference.\n",
    "#### we build the real train dataset in an object called train_x which will contain actual image pixels( i.e. the real images themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deepak/Saquib/CNNs\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age_detection.ipynb\r\n",
      "Age_detection_sub1.csv\r\n",
      "Age_detection_sub2.csv\r\n",
      "Converting MNIST images to RGB.ipynb\r\n",
      "data\r\n",
      "first_try.h5\r\n",
      "mnist.pkl.gz\r\n",
      "Rough Notebook.ipynb\r\n",
      "sub1.csv\r\n",
      "sub2.csv\r\n",
      "sub3.csv\r\n",
      "Transfer Learning.ipynb\r\n",
      "vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path = 'data/train/Train/'+image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 78, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = imread(image_path)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe0f24c2690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANAAAAD8CAYAAAAGyio5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nomIP1BeoomIP1BeoomIP/H858jyJaawcoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0f3547d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-78dec1ddfcc9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-78dec1ddfcc9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print train.class[i]\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print train.class[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>19906</td>\n",
       "      <td>19906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19906</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>25628.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID   Class\n",
       "count       19906   19906\n",
       "unique      19906       3\n",
       "top     25628.jpg  MIDDLE\n",
       "freq            1   10804"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUNG\n"
     ]
    }
   ],
   "source": [
    "print train.Class[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          377.jpg\n",
      "1        17814.jpg\n",
      "2        21283.jpg\n",
      "3        16496.jpg\n",
      "4         4487.jpg\n",
      "5         6283.jpg\n",
      "6        23495.jpg\n",
      "7         7100.jpg\n",
      "8         6028.jpg\n",
      "9        22617.jpg\n",
      "10       11177.jpg\n",
      "11        2462.jpg\n",
      "12       24116.jpg\n",
      "13       17689.jpg\n",
      "14         728.jpg\n",
      "15        3003.jpg\n",
      "16       14408.jpg\n",
      "17        6679.jpg\n",
      "18       15152.jpg\n",
      "19       24784.jpg\n",
      "20        9970.jpg\n",
      "21       22550.jpg\n",
      "22         150.jpg\n",
      "23        7379.jpg\n",
      "24       15387.jpg\n",
      "25        2336.jpg\n",
      "26        9603.jpg\n",
      "27        4025.jpg\n",
      "28       17696.jpg\n",
      "29       17552.jpg\n",
      "           ...    \n",
      "19876    11988.jpg\n",
      "19877     9407.jpg\n",
      "19878    25426.jpg\n",
      "19879    16609.jpg\n",
      "19880    18746.jpg\n",
      "19881    25714.jpg\n",
      "19882    14939.jpg\n",
      "19883    10025.jpg\n",
      "19884     6149.jpg\n",
      "19885     9733.jpg\n",
      "19886    22630.jpg\n",
      "19887    11803.jpg\n",
      "19888    10812.jpg\n",
      "19889     7038.jpg\n",
      "19890     3021.jpg\n",
      "19891     4727.jpg\n",
      "19892     7979.jpg\n",
      "19893    26159.jpg\n",
      "19894     2040.jpg\n",
      "19895    25347.jpg\n",
      "19896    15100.jpg\n",
      "19897    26182.jpg\n",
      "19898      742.jpg\n",
      "19899     5318.jpg\n",
      "19900    25514.jpg\n",
      "19901     2482.jpg\n",
      "19902    20085.jpg\n",
      "19903    19663.jpg\n",
      "19904    10132.jpg\n",
      "19905     9896.jpg\n",
      "Name: ID, Length: 19906, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print train.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0e81964c22d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m      \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/train/Train/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m      \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m      \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m      \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m      \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/misc/pilutil.pyc\u001b[0m in \u001b[0;36mimresize\u001b[0;34m(arr, size, interp, mode)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lanczos'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bicubic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cubic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m     \u001b[0mimnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample)\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unsupported resampling filter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " ######### Making train_x ############33\n",
    "    \n",
    "from scipy.misc import imresize\n",
    "\n",
    "temp = []\n",
    "\n",
    "for image_name in train.ID:\n",
    "      image_path = 'data/train/Train/'+image_name\n",
    "      image = imread(image_path)\n",
    "      image = imresize(image,(32,32))\n",
    "      image = image.astype('float32') \n",
    "      temp.append(image)\n",
    "        \n",
    "train_x = np.stack(temp)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## Making test_x ############\n",
    "temp = []\n",
    "for image_name in test.ID:\n",
    "     image_path = 'data/test/Test/'+image_name\n",
    "     image = imread(image_path) #########  Here the function \" imread \" returns a numpy array , given a .jpg image  #######\n",
    "     image = imresize(image,(32,32))\n",
    "     temp.append(image.astype('float32')) \n",
    "    \n",
    "test_x = np.stack(temp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19906, 32, 32, 3) (6636, 32, 32, 3)\n",
      "(19906, 3072) (6636, 3072)\n"
     ]
    }
   ],
   "source": [
    "print train_x.shape,test_x.shape\n",
    "\n",
    "train_x = np.reshape(train_x,(19906,32*32*3))\n",
    "test_x = np.reshape(test_x,(6636,32*32*3))\n",
    "\n",
    "print train_x.shape,test_x.shape\n",
    "\n",
    "\n",
    "train_x = train_x / 255\n",
    "test_x = test_x/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MIDDLE    0.542751\n",
       "YOUNG     0.336883\n",
       "OLD       0.120366\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################# Let's see what is the distribution of ages in the training dataset ###################\n",
    "train.Class.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>23251.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID\n",
       "count        6636\n",
       "unique       6636\n",
       "top     23251.jpg\n",
       "freq            1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## So we see that the majority people in the training data are MIDDLE aged ###########\n",
    "##### so a naive submission will be that since the majority of the people in the training set are MIDDLE aged , so we assume\n",
    "######### all test data people are also MIDDLE aged.\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of              ID\n",
       "0     25321.jpg\n",
       "1       989.jpg\n",
       "2     19277.jpg\n",
       "3     13093.jpg\n",
       "4      5367.jpg\n",
       "5     19851.jpg\n",
       "6     10384.jpg\n",
       "7     24567.jpg\n",
       "8      2029.jpg\n",
       "9      4956.jpg\n",
       "10    26131.jpg\n",
       "11    17407.jpg\n",
       "12    11136.jpg\n",
       "13    23038.jpg\n",
       "14     6333.jpg\n",
       "15    25076.jpg\n",
       "16    22102.jpg\n",
       "17    12396.jpg\n",
       "18    24719.jpg\n",
       "19    21354.jpg\n",
       "20    15362.jpg\n",
       "21    10415.jpg\n",
       "22     5295.jpg\n",
       "23     5531.jpg\n",
       "24    16784.jpg\n",
       "25    22442.jpg\n",
       "26    16794.jpg\n",
       "27    11261.jpg\n",
       "28    11540.jpg\n",
       "29    24286.jpg\n",
       "...         ...\n",
       "6606  13841.jpg\n",
       "6607   8999.jpg\n",
       "6608  15780.jpg\n",
       "6609  10241.jpg\n",
       "6610  13006.jpg\n",
       "6611  20322.jpg\n",
       "6612  24054.jpg\n",
       "6613  21310.jpg\n",
       "6614  24707.jpg\n",
       "6615   3292.jpg\n",
       "6616  18392.jpg\n",
       "6617   7731.jpg\n",
       "6618  26522.jpg\n",
       "6619  25614.jpg\n",
       "6620  11283.jpg\n",
       "6621   1485.jpg\n",
       "6622  15057.jpg\n",
       "6623   9582.jpg\n",
       "6624  10634.jpg\n",
       "6625  15146.jpg\n",
       "6626  16049.jpg\n",
       "6627  13461.jpg\n",
       "6628    147.jpg\n",
       "6629  22636.jpg\n",
       "6630   6512.jpg\n",
       "6631   1876.jpg\n",
       "6632  14940.jpg\n",
       "6633   3638.jpg\n",
       "6634    376.jpg\n",
       "6635   9357.jpg\n",
       "\n",
       "[6636 rows x 1 columns]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Class']='MIDDLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6636</td>\n",
       "      <td>6636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6636</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>23251.jpg</td>\n",
       "      <td>MIDDLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>6636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID   Class\n",
       "count        6636    6636\n",
       "unique       6636       1\n",
       "top     23251.jpg  MIDDLE\n",
       "freq            1    6636"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################### FIRST SUBISSION ################\n",
    "\n",
    "test.to_csv('sub1.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################## SECOND MODEL : USING  SIMPLE NEURAL NETWORKS (SNNs)  FOR CLASSIFICATION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LabelEncoder() function is in sklearn.preprocessing.LableEcoder class\n",
    "# The function of LabelEncoder is to convert a list of categorical values to encoded numerical list.\n",
    "# The function of the keras.utils.np_utils.to_categorical() is to convert the above encoded numerical list to one-hot \n",
    "# representation matrix\n",
    "\n",
    "lb = LabelEncoder()\n",
    "train_y = lb.fit_transform(train.Class)\n",
    "train_y = keras.utils.np_utils.to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what we did above ##\n",
    "\n",
    "#######################\n",
    "#######################\n",
    "# We can do this by first encoding the strings consistently to integers using the scikit-learn class LabelEncoder. \n",
    "# Then convert the vector of integers to a one hot encoding using the Keras function to_categorical().\n",
    "#######################\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### BUILDING THE MODEL ITSELF #########\n",
    "\n",
    "###### Parameters for the model ############\n",
    "input_num_units = (32, 32, 3)\n",
    "hidden_num_units = 500\n",
    "output_num_classes = 3\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "###### importing keras modules to be able to build the model ###########\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim=hidden_num_units, input_dim=32*32*3,activation='relu'))\n",
    "model.add(Dense(output_dim=output_num_classes,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 500)           1536500     dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 3)             1503        dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,538,003\n",
      "Trainable params: 1,538,003\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19906, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "19906/19906 [==============================] - 7s - loss: 0.8928 - acc: 0.5766     \n",
      "Epoch 2/5\n",
      "19906/19906 [==============================] - 7s - loss: 0.8432 - acc: 0.6047     \n",
      "Epoch 3/5\n",
      "19906/19906 [==============================] - 8s - loss: 0.8251 - acc: 0.6158     \n",
      "Epoch 4/5\n",
      "19906/19906 [==============================] - 7s - loss: 0.8118 - acc: 0.6290     \n",
      "Epoch 5/5\n",
      "19906/19906 [==============================] - 7s - loss: 0.8061 - acc: 0.6263     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21eb7be750>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, batch_size=batch_size,nb_epoch=5,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15924 samples, validate on 3982 samples\n",
      "Epoch 1/5\n",
      "15924/15924 [==============================] - 6s - loss: 0.8053 - acc: 0.6253 - val_loss: 0.7849 - val_acc: 0.6366\n",
      "Epoch 2/5\n",
      "15924/15924 [==============================] - 6s - loss: 0.7953 - acc: 0.6370 - val_loss: 0.7861 - val_acc: 0.6366\n",
      "Epoch 3/5\n",
      "15924/15924 [==============================] - 7s - loss: 0.7933 - acc: 0.6355 - val_loss: 0.7755 - val_acc: 0.6522\n",
      "Epoch 4/5\n",
      "15924/15924 [==============================] - 6s - loss: 0.7905 - acc: 0.6375 - val_loss: 0.7958 - val_acc: 0.6351\n",
      "Epoch 5/5\n",
      "15924/15924 [==============================] - 6s - loss: 0.7875 - acc: 0.6409 - val_loss: 0.7852 - val_acc: 0.6479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7870f3ec50>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=batch_size,nb_epoch=epochs,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6560/6636 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "prediction = model.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = lb.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Class']=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('sub2.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## now using our final model ....CNN to classify these images ages \n",
    "train_x = train_x.reshape(-1,32,32,3)\n",
    "test_x = test_x.reshape(-1,32,32,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19906, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 2, 2, input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, 2, 2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, 2, 2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(96))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19906/19906 [==============================] - 26s - loss: 0.6805 - acc: 0.7038    \n",
      "Epoch 2/10\n",
      "19906/19906 [==============================] - 25s - loss: 0.6663 - acc: 0.7140    \n",
      "Epoch 3/10\n",
      "19906/19906 [==============================] - 25s - loss: 0.6444 - acc: 0.7255    \n",
      "Epoch 4/10\n",
      "19906/19906 [==============================] - 26s - loss: 0.6252 - acc: 0.7356    \n",
      "Epoch 5/10\n",
      "19906/19906 [==============================] - 26s - loss: 0.6140 - acc: 0.7395    \n",
      "Epoch 6/10\n",
      "19906/19906 [==============================] - 26s - loss: 0.5960 - acc: 0.7449    \n",
      "Epoch 7/10\n",
      "19906/19906 [==============================] - 25s - loss: 0.5849 - acc: 0.7532    \n",
      "Epoch 8/10\n",
      "19906/19906 [==============================] - 25s - loss: 0.5714 - acc: 0.7568    \n",
      "Epoch 9/10\n",
      "19906/19906 [==============================] - 26s - loss: 0.5604 - acc: 0.7618    \n",
      "Epoch 10/10\n",
      "19906/19906 [==============================] - 26s - loss: 0.5514 - acc: 0.7681    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21a0d0dad0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=batch_size,nb_epoch=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6624/6636 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "prediction = model.predict_classes(test_x)\n",
    "prediction = lb.inverse_transform(prediction)\n",
    "test['Class']=prediction\n",
    "test.to_csv('sub3.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################## Building a more robust CNN model using ImageDataGenerator() ####################33\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,zca_whitening=True,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "datagen.fit(train_x)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 2, 2, input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, 2, 2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, 2, 2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(96))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# the model so far outputs 3D feature maps (height, width, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19906/19906 [==============================] - 78s - loss: 0.9708 - acc: 0.5377    \n",
      "Epoch 2/10\n",
      "19906/19906 [==============================] - 79s - loss: 0.9235 - acc: 0.5685    \n",
      "Epoch 3/10\n",
      "19906/19906 [==============================] - 78s - loss: 0.9034 - acc: 0.5888    \n",
      "Epoch 4/10\n",
      "19906/19906 [==============================] - 78s - loss: 0.8772 - acc: 0.6055    \n",
      "Epoch 5/10\n",
      "19906/19906 [==============================] - 78s - loss: 0.8590 - acc: 0.6103    \n",
      "Epoch 6/10\n",
      "19906/19906 [==============================] - 79s - loss: 0.8501 - acc: 0.6137    \n",
      "Epoch 7/10\n",
      "19906/19906 [==============================] - 79s - loss: 0.8325 - acc: 0.6215    \n",
      "Epoch 8/10\n",
      "19906/19906 [==============================] - 78s - loss: 0.8291 - acc: 0.6252    \n",
      "Epoch 9/10\n",
      "19906/19906 [==============================] - 78s - loss: 0.8178 - acc: 0.6349    \n",
      "Epoch 10/10\n",
      "19906/19906 [==============================] - 78s - loss: 0.8044 - acc: 0.6396    \n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        datagen.flow(train_x, train_y, batch_size=128),\n",
    "        samples_per_epoch=train_x.shape[0],\n",
    "        nb_epoch=10)\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### so now we use  some transfer learning ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 50, 50, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, 50, 50, 64)    1792        input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, 50, 50, 64)    36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 25, 25, 64)    0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, 25, 25, 128)   73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, 25, 25, 128)   147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 12, 12, 128)   0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, 12, 12, 256)   295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, 12, 12, 256)   590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, 12, 12, 256)   590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 6, 6, 256)     0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, 6, 6, 512)     1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, 6, 6, 512)     2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, 6, 6, 512)     2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 3, 3, 512)     0           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, 3, 3, 512)     2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, 3, 3, 512)     2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, 3, 3, 512)     2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, 1, 1, 512)     0           block5_conv3[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 50, 50, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, 50, 50, 64)    1792        input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, 50, 50, 64)    36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 25, 25, 64)    0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, 25, 25, 128)   73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, 25, 25, 128)   147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 12, 12, 128)   0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, 12, 12, 256)   295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, 12, 12, 256)   590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, 12, 12, 256)   590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 6, 6, 256)     0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, 6, 6, 512)     1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, 6, 6, 512)     2359808     block4_conv1[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 5,275,456\n",
      "Trainable params: 5,275,456\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "model = VGG16(weights='imagenet', include_top=False,input_shape = (50,50,3))\n",
    "model.summary()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of               ID   Class\n",
       "0        377.jpg  MIDDLE\n",
       "1      17814.jpg   YOUNG\n",
       "2      21283.jpg  MIDDLE\n",
       "3      16496.jpg   YOUNG\n",
       "4       4487.jpg  MIDDLE\n",
       "5       6283.jpg  MIDDLE\n",
       "6      23495.jpg   YOUNG\n",
       "7       7100.jpg   YOUNG\n",
       "8       6028.jpg   YOUNG\n",
       "9      22617.jpg     OLD\n",
       "10     11177.jpg   YOUNG\n",
       "11      2462.jpg  MIDDLE\n",
       "12     24116.jpg  MIDDLE\n",
       "13     17689.jpg  MIDDLE\n",
       "14       728.jpg  MIDDLE\n",
       "15      3003.jpg  MIDDLE\n",
       "16     14408.jpg     OLD\n",
       "17      6679.jpg   YOUNG\n",
       "18     15152.jpg     OLD\n",
       "19     24784.jpg  MIDDLE\n",
       "20      9970.jpg   YOUNG\n",
       "21     22550.jpg     OLD\n",
       "22       150.jpg   YOUNG\n",
       "23      7379.jpg  MIDDLE\n",
       "24     15387.jpg  MIDDLE\n",
       "25      2336.jpg   YOUNG\n",
       "26      9603.jpg  MIDDLE\n",
       "27      4025.jpg     OLD\n",
       "28     17696.jpg  MIDDLE\n",
       "29     17552.jpg   YOUNG\n",
       "...          ...     ...\n",
       "19876  11988.jpg     OLD\n",
       "19877   9407.jpg     OLD\n",
       "19878  25426.jpg   YOUNG\n",
       "19879  16609.jpg   YOUNG\n",
       "19880  18746.jpg   YOUNG\n",
       "19881  25714.jpg  MIDDLE\n",
       "19882  14939.jpg  MIDDLE\n",
       "19883  10025.jpg   YOUNG\n",
       "19884   6149.jpg     OLD\n",
       "19885   9733.jpg   YOUNG\n",
       "19886  22630.jpg  MIDDLE\n",
       "19887  11803.jpg  MIDDLE\n",
       "19888  10812.jpg   YOUNG\n",
       "19889   7038.jpg   YOUNG\n",
       "19890   3021.jpg  MIDDLE\n",
       "19891   4727.jpg   YOUNG\n",
       "19892   7979.jpg  MIDDLE\n",
       "19893  26159.jpg  MIDDLE\n",
       "19894   2040.jpg  MIDDLE\n",
       "19895  25347.jpg  MIDDLE\n",
       "19896  15100.jpg   YOUNG\n",
       "19897  26182.jpg  MIDDLE\n",
       "19898    742.jpg     OLD\n",
       "19899   5318.jpg  MIDDLE\n",
       "19900  25514.jpg  MIDDLE\n",
       "19901   2482.jpg  MIDDLE\n",
       "19902  20085.jpg   YOUNG\n",
       "19903  19663.jpg  MIDDLE\n",
       "19904  10132.jpg  MIDDLE\n",
       "19905   9896.jpg  MIDDLE\n",
       "\n",
       "[19906 rows x 2 columns]>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " ############ Making train_x ##############   \n",
    "from keras.preprocessing import image\n",
    "temp = []\n",
    "\n",
    "for i,image_name in enumerate(train.ID):\n",
    "      image_path = 'data/train/Train/'+image_name\n",
    "      im = image.load_img(image_path,target_size = (50,50))\n",
    "      im = image.img_to_array(im)\n",
    "      #im = np.expand_dims(im,axis =0)\n",
    "      #im = im.astype('float32') \n",
    "      temp.append(im)\n",
    "        \n",
    "train_x = np.array(temp)     \n",
    "\n",
    "\n",
    "######### Making test_x ############\n",
    "temp = []\n",
    "for image_name in test.ID:\n",
    "     image_path = 'data/test/Test/'+image_name\n",
    "     im = image.load_img(image_path,target_size = (50,50))\n",
    "     im = image.img_to_array(im)\n",
    "     #im = np.expand_dims(im,axis =0)\n",
    "     #im = im.astype('float32') \n",
    "     temp.append(im)    \n",
    "     \n",
    "test_x = np.array(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_x = preprocess_input(train_x)\n",
    "\n",
    "features_train=model.predict(train_x)\n",
    "# Extracting features from the train dataset using the VGG16 pre-trained model\n",
    "test_x = preprocess_input(test_x)\n",
    "features_test=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19906, 50, 50, 3)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6636, 1, 1, 512)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10191872 into shape (19906,18432)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-24e04d3d0eda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19906\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 10191872 into shape (19906,18432)"
     ]
    }
   ],
   "source": [
    "train_x = features_train.reshape(19906,6*6*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = features_test.reshape(6636,6*6*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y=np.asarray(train['Class'])\n",
    "# performing one-hot encoding for the target variable\n",
    "\n",
    "train_y=pd.get_dummies(train_y)\n",
    "train_y=np.array(train_y)\n",
    "# creating training and validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_x,train_y,test_size=0.3, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Now adding a layer a the end layers according to our data ############\n",
    "\n",
    "# creating a mlp model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(200, input_dim=512, activation='relu'))\n",
    "keras.layers.core.Dropout(0.8, noise_shape=None, seed=None)\n",
    "\n",
    "model.add(Dense(64,input_dim=200,activation='sigmoid'))\n",
    "keras.layers.core.Dropout(0.8, noise_shape=None, seed=None)\n",
    "\n",
    "\n",
    "model.add(Dense(3,input_dim = 64,activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13934 samples, validate on 5972 samples\n",
      "Epoch 1/20\n",
      "13934/13934 [==============================] - ETA: 0s - loss: 0.8560 - acc: 0.615 - 1s - loss: 0.8543 - acc: 0.6160 - val_loss: 0.8016 - val_acc: 0.6480\n",
      "Epoch 2/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.7588 - acc: 0.6636 - val_loss: 0.7792 - val_acc: 0.6525\n",
      "Epoch 3/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.7133 - acc: 0.6895 - val_loss: 0.7558 - val_acc: 0.6693\n",
      "Epoch 4/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.6645 - acc: 0.7119 - val_loss: 0.7490 - val_acc: 0.6750\n",
      "Epoch 5/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.6076 - acc: 0.7415 - val_loss: 0.7739 - val_acc: 0.6680\n",
      "Epoch 6/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.5657 - acc: 0.7639 - val_loss: 0.7571 - val_acc: 0.6815\n",
      "Epoch 7/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.5042 - acc: 0.7911 - val_loss: 0.7655 - val_acc: 0.6824\n",
      "Epoch 8/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.4605 - acc: 0.8163 - val_loss: 0.7678 - val_acc: 0.6916\n",
      "Epoch 9/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.4114 - acc: 0.8370 - val_loss: 0.7866 - val_acc: 0.6832\n",
      "Epoch 10/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.3673 - acc: 0.8570 - val_loss: 0.8226 - val_acc: 0.6795\n",
      "Epoch 11/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.3198 - acc: 0.8763 - val_loss: 0.8605 - val_acc: 0.6822\n",
      "Epoch 12/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.2854 - acc: 0.8932 - val_loss: 0.8739 - val_acc: 0.6859\n",
      "Epoch 13/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.2537 - acc: 0.9077 - val_loss: 0.9185 - val_acc: 0.6854\n",
      "Epoch 14/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.2236 - acc: 0.9208 - val_loss: 0.9582 - val_acc: 0.6832\n",
      "Epoch 15/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.2023 - acc: 0.9283 - val_loss: 0.9986 - val_acc: 0.6795\n",
      "Epoch 16/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.1827 - acc: 0.9366 - val_loss: 1.0416 - val_acc: 0.6800\n",
      "Epoch 17/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.1547 - acc: 0.9480 - val_loss: 1.0821 - val_acc: 0.6835\n",
      "Epoch 18/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.1357 - acc: 0.9551 - val_loss: 1.1402 - val_acc: 0.6820\n",
      "Epoch 19/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.1230 - acc: 0.9600 - val_loss: 1.1786 - val_acc: 0.6803\n",
      "Epoch 20/20\n",
      "13934/13934 [==============================] - 0s - loss: 0.1117 - acc: 0.9663 - val_loss: 1.2496 - val_acc: 0.6731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0afcfc590>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, nb_epoch=20, batch_size=100,validation_data=(X_valid,Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
